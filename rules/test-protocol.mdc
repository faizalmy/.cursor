---
globs: tests/**/*.test.ts
alwaysApply: false
---
You will create a comprehensive, production-grade test suite through systematic analysis and incremental development. This process ensures complete coverage, maintainable code, and precise failure reporting for any given codebase or functionality.

### **Phase 1: CODEBASE ANALYSIS**
Begin by conducting a thorough analysis of the target code:

**1.1 Module Identification**
- Examine all files, classes, functions, and methods in scope
- Identify public interfaces, private methods, and external dependencies
- Map data flow and component interactions
- Document entry points and critical execution paths

**1.2 Testability Assessment**
- Categorize components by test complexity: **Simple** (pure functions), **Moderate** (stateful classes), **Complex** (integration points)
- Identify external dependencies requiring mocking or stubbing
- Flag asynchronous operations, file I/O, network calls, and database interactions
- Note error-prone areas and edge case scenarios

**1.3 Test Requirement Matrix**
Create a structured breakdown:
```
MODULE: [module_name]
├── Unit Tests Required: [count]
├── Integration Tests Required: [count]
├── Edge Cases Identified: [list]
├── Dependencies to Mock: [list]
└── Priority Level: [High/Medium/Low]
```

### 1.4 External Dependencies Testing Best Practices

- **Unit Tests:** Always mock or fake all external dependencies (databases, APIs, services, third-party libraries). Never use real external services in unit tests.
- **Mocking Strategy:** Use appropriate mocking libraries for your framework (e.g., Mockito, Jest mocks, PHPUnit mocks, unittest.mock, etc.)
- **Dependency Injection:** Inject all external dependencies into classes and functions for testability
- **Integration Tests:** Use test doubles, fakes, or isolated test environments for external services
- **CI/CD:** Ensure test environments have proper configuration for integration tests
- **Test Structure:** Never let external service failures cause test flakiness—always isolate with mocks/fakes

### **Phase 2: BATCH PLANNING**
Organize test development into logical, manageable batches:

**2.1 Batch Criteria**
- **Batch Size**: Maximum 5-8 test files per batch to maintain focus
- **Dependency Order**: Lower-level utilities before higher-level consumers
- **Complexity Grouping**: Simple tests before complex integration scenarios
- **Risk Prioritization**: Critical functionality first, edge cases second

**2.2 Batch Structure Template**
```
BATCH #: [number] - [descriptive_name]
├── Scope: [specific components/functionality]
├── Test Types: [unit/integration/e2e]
├── Dependencies: [required mocks/fixtures]
├── Estimated Tests: [count]
├── Prerequisites: [previous batches required]
└── Success Criteria: [coverage goals]
```

### **Phase 3: TEST IMPLEMENTATION**
For each approved batch, create production-grade tests following these standards:

**3.1 Test File Structure**
Begin every test file with:
```
/**
 * TEST SUITE: [module_name]
 * PURPOSE: [clear description of what functionality is being tested]
 * SCOPE: [specific functions/methods covered]
 * DEPENDENCIES: [external requirements, mocks, fixtures]
 * LAST UPDATED: [date]
 */
```

**3.2 Individual Test Standards**
Each test must follow the **AAA Pattern** (Arrange, Act, Assert):
```
// TEST: should_[expected_behavior]_when_[specific_condition]
test('should_return_valid_result_when_input_is_positive_integer', () => {
    // ARRANGE: Set up test data and environment
    const input = 42;
    const expected = 'positive';

    // ACT: Execute the function under test
    const result = classifyNumber(input);

    // ASSERT: Verify expected outcome with specific error messages
    expect(result).toBe(expected,
        `Expected classifyNumber(${input}) to return '${expected}', but got '${result}'`);
});
```

**3.3 Required Test Categories**
For each function/method, implement:

- **Happy Path Tests**: Valid inputs producing expected outputs
- **Boundary Tests**: Edge values (null, undefined, empty, max/min values)
- **Error Handling Tests**: Invalid inputs triggering appropriate error responses
- **State Tests**: For stateful objects, test state changes and side effects
- **Integration Tests**: Component interactions and data flow validation

**3.4 Documentation Requirements**
- **Function Purpose**: Comment explaining what each test validates
- **Input Explanation**: Document test data choices and significance
- **Expected Behavior**: Clear description of anticipated results
- **Failure Scenarios**: Specific error conditions and expected responses
- **Mock Justification**: Why specific dependencies are mocked and how

### **Phase 4: COVERAGE VERIFICATION**
After each batch completion:

**4.1 Coverage Analysis**
- Verify all public methods have corresponding tests
- Confirm error handling paths are exercised
- Validate integration points are tested
- Check that edge cases are comprehensively covered

**4.2 Quality Checklist**
- [ ] All tests follow consistent naming conventions
- [ ] Each test has single, clear responsibility
- [ ] Failure messages provide actionable debugging information
- [ ] Tests are independent and can run in any order
- [ ] External dependencies are properly isolated
- [ ] Test data is meaningful and representative

### **Phase 5: IMPLEMENTATION ROADMAP**
Conclude with a sequential deployment plan:

```
IMPLEMENTATION SEQUENCE:
1. [Setup test environment and dependencies]
2. [Implement Batch 1: Core utilities]
3. [Implement Batch 2: Business logic]
4. [Implement Batch 3: Integration scenarios]
...
n. [Final validation and coverage report]
```

### **Execution Protocol**
- **Incremental Approval**: Present each batch plan for approval before implementation
- **Quality Gates**: Each batch must pass quality checklist before proceeding
- **Documentation First**: Always explain test purpose before showing implementation
- **Failure Analysis**: When tests fail, provide exact location and remediation steps
- **Continuous Validation**: Verify previous batches remain functional as new tests are added

### **Output Requirements**
- Present analysis findings in structured format
- Show batch plans with clear scope and dependencies
- Provide complete, runnable test code with comprehensive documentation
- Include specific assertions with detailed failure messages
- Deliver implementation roadmap with clear sequencing
- Show batch plans with clear scope and dependencies
- Provide complete, runnable test code with comprehensive documentation
- Include specific assertions with detailed failure messages
- Deliver implementation roadmap with clear sequencing